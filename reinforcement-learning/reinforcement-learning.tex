\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\author{Mario Dohr}
\title{Reinforcement Learning}
\begin{document}
\section{Introduction}
What is Reinforcement Learning? RL is the computational approach to learning from interactions.
RL problems involvle learning "what to do":
\begin{itemize}
\item by mapping states to actions
\item while maximizing total reward
\end{itemize}

\paragraph{Three central Characteristics:}
\begin{itemize}
\item Closed-loop problems: actions of agents influence its later observations
\item No direct instructions: the agent is not told which action to take.
\item No initial knowledge: the agent has to figure out the consquences of its actions.
\end{itemize}

\paragraph{Differences to Supervised- and Unsupervised learning}
\begin{itemize}
\item Difference to Suppervised learning: 1. Agent must learn from own experience. 2. No supervisor tell what to do
\item Difference to unsupervised learning: 1. No predefined data. 2. Agent tries to maximize reward.
\end{itemize}

\paragraph{Reinforcement}
Maximize reward.
\begin{itemize}
\item \textbf{Given:} an environment and a reward signal
\item \textbf{Find:} a behavior that maximizes total reward over time
\end{itemize}
The agent takes action in the environment and gets back a state and a reward. Challenge is finding actions by exploring, exploit knowledge about good actions to maximize reward.

\paragraph{Challenges in RL}
\begin{itemize}
\item How maximize reward? We need to exploit our knowledge about the past.
\item How find highly rewarded actions? We need to explore the environment.
\end{itemize}
\textbf{Dilemma:} We have a trade-off between \textbf{Explore} and \textbf{Exploit}:
\begin{itemize}
\item to obtain reward an agent must favor actions that have proven to be benificial in the past (Exploitation).
\item to discover such actions, an agent has to try actions that it has not selected before (Exploration).
\end{itemize}


\subsection{Examples}
\begin{itemize}
\item a animal learn to walk.
\item playing chess
\item trash collection robots: has to decide search more trash or go to recharge.
\item Computer games
\item OpenAI Gym is a python library for reinforcement learning
\end{itemize}
\textbf{Commonalities}
\begin{itemize}
\item interaction between agent and environment
\item agents seek to achieve a goal in their environment, despite uncertainty
\item actions affect the future state of the environment
\item has to take into to account indirect, delayed consequences of actions
\item consequences of actions can't be fully predicted
\item agents use experience
\item interaction with environment is essential
\end{itemize}

\subsection{Elements of Reinforcement Learning}
\begin{itemize}
\item \textbf{Environment}: for example: the universe, one street, coffee machine, chess board ...
\item \textbf{States}: represents the environments current condition: a position, temperature, etc.
\item \textbf{Actions}: for each state there is a set of actions. E.g. turning steering wheel, move in a particular direction, ...
\item \textbf{Policy}: A policy completely determines its behavior. It defines which action an agent can take in a given state. E.g. if car leaves street, seer in the other direction, if pressure to high, decrease current to heating element, move to a position with high expected future reward. Is a property of the agent. An agent has a specific policy at a specific time.
\item \textbf{Reward signal}: Is given out from the environment and encodes how good the agent is doing currently. Given from environment, not the agents knowledge. It is the primary basis for altering the policy. E.g.: winning or loosing the game, car stays on the street, does not crash. 
\item \textbf{Value functions}: is compressed knowledge about the future, it encodes an agent's experience. E.g.: eat cake, feel good. Turn steering wheel D degrees avoid a crash.
\end{itemize}

\paragraph{Policy}
The behavior of an agent is called a policy. The policy:
\begin{itemize}
\item maps a state to a probability distribution over actions
\item samples from this distribution to select an action
\end{itemize}
We can say an RL agent follows a policy (behaves a certain way) and updates its policy (to try and maximize reward).
The policy completely determines its behavior, deciding which action to take at a given state.
The policy is that, what the agent should learn???

\paragraph{Policy implementation} Is a mapping from states to actions. Could be
\begin{itemize}
\item a lookup table
\item a simple function
\item a search process
\item a DNN
\item a combination
\end{itemize}

\paragraph{Reward Signal}. Defines the goal in a RL problem. The reward:
\begin{itemize}
\item is a single real number
\item is perceived by the agent at each time step
\item defines what is good and what is bad
\item is immediate and defines features of the problem
\item primary basis for changing the policy
\end{itemize}

\paragraph{Value Function} Defines what is good in the long run. 
\begin{itemize}
\item is the total amount of reward an agent can expect to accumulate in the future starting from that state
\item usually an estimate
\item often used to choose an action
\end{itemize}

\paragraph{Reward vs. Value}
\begin{itemize}
\item rewards are immediate, part of environment
\item values are predictions of future reward, part of the agent
\item rewards are used, in order to estimate value
\item the only purpose of estimation values is to get more reward in the long run
\item most RL algos focus on efficient value estimation
\item a state might yield a low immediate reward but still have a high value because it is usually followed by benificial states.
\end{itemize}

\paragraph{Model environment}
Model-based methods vs model-free methods.

\paragraph{Boundaries}
Boundary between Agent and Environment?  \textcolor{red}{Fill summary of Example with the fish} this is temporal-difference-learning.

\subsection{Example: Gridworld Environment}
\begin{itemize}
\item \textbf{States:} we have 11 states: where is the fish?
\item \textbf{Actions:} In all states we have the same actions. The fish can move. When he moves left in the left bottom the state doesn't change.
\item \textbf{Loop:} the agent takes an action and receives a new state and a reward.
\item \textbf{How get food and survive?} We set up a table with the states and the values for the value function.
\end{itemize}

\textcolor{red}{Lookup gridworld learning}

\section{Multi-armed Bandits}
Here we are in a so called \textit{nonassociative} setting, this is we only have one situation or state.

\subsection{k-armed bandits}
Is inspired by the "slot machines" in casinos. 
The problem is the following: we have $k$ different options or choices for an action and receive a reward from a probability distribution. We try to maximize the return over time. Each of the action we can take have an expected reward:
\[ q_{*}(a) = \mathbb{E}[R_t | A_t = a ] \]
If we would know this, we always would choose the action with the highest reward. But we don't know this, wie only have the estimates $Q_t(a)$, which is the estimated value of action $a$ at time step $t$. We want $Q_t(a)$ close to $q_{*}(a)$.

At every time step, their is one action $a$ with the highest value, this is the greedy action. If we choose this action, we are \textit{exploiting} our knowledge about the values of the actions, but we don't gain new knowledge. If we choose a non-greedy action, we are exploring and gain knowledge about the values of action. 
There is always a trade-off between \textit{explore} and \textit{exploit}. The need to balance exploration
and exploitation is a distinctive challenge that arises in reinforcement learning.

\subsection{Action-value Methods}
We are looking for methods to estimate the value of actions. The true value of an action is the mean reward when selected. We can estimate this with:
\[ Q_t(a) = \frac{\sum_{i_1}^{t-1}R_t \; 1_{A_t=a}}{\sum_{i=1}^{t-1} 1_{A_i=a} } \]
By the law of large numbers, $Q_t(a)$ converges to $q_*(a)$.
The greedy action selection method is
\[ A_t = \arg\max_a Q_t(a) \]
This is called the \textit{sample-average} method. This is maybe not the best method. Instead of this you could randomly select another method to explore new knowledge and therefore maybe get higher rewards in the future.
This is called the $\epsilon$\textit{-greedy} method: With probability $(1-\epsilon)$ the agent take the greedy action and with probability $\epsilon$ it select an action randomly.
This method also ensures then $Q_t(a) \rightarrow q_*(a)$ for all $a$ and the probability of selecting the optimal solution converges to greater than $1-\epsilon$.

\paragraph{Implementation}
$Q_{t+1}$ can be computed from $Q_t$:
\[ Q_{t+1} = \frac{1}{t}\sum_{i=1}^t R_i \]
\[ = \frac{1}{t}(R_t + (t-1)\frac{1}{(t-1)}\sum_{i=1}^{t-1} R_i)\]
\[ = Q_t + \frac{1}{t}(R_t + Q_t) \]
This update rule occurs often in RL. The general form is
\[ newEstimate = oldEstimate + stepSize[target - old]\]

\subsection{Optimistic initial values}
All methods we discussed so far, depends on the initial values to some extend. If we have prior knowledge we can use that to get better results. If we don't have prior knowledge but set high (optimistic) values (higher than the highest possible reward), then the algorithm starts exploring. Because when an action is taken, the reward is lower than the current value, so the value of the action is reduced and has lower values then the other actions. 

\subsection{Upper-Confident bound action selection}
$\epsilon$-greedy selection selects a non-greedy action but randomly with no preference. 
It would be better to select among the non-greedy actions according to their potential of being optimal.
\[ A_t = \arg \max_a \Bigl[ Q_t(a) + \sqrt{\frac{\ln t}{N_t(a)}} \Bigr ] \]
The square root term represents something like a variance or uncertainty in the estimates of $a's$ values. 
If $a$ is chosen often, this term gets reduced. The $\ln$ ensures, that the increases get smaller over time, if $a$ is not selected, but is still unbounded.
\end{document}